{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION DES MALADIES DES FEVES DE CACAO EN UTILISANT LES ALGORITHMES CONVNEXT ET SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I-Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import hashlib\n",
    "\n",
    "list_disease = [\"Sana\", \"Fito\", \"Monilia\"]\n",
    "for disease in list_disease:\n",
    "    # Chemins d'accès\n",
    "    images_dir = f\"Cocoa/{disease}\"  # Dossier contenant les images et les annotations\n",
    "    output_dir = f\"InterestData/\"  # Dossier de sortie\n",
    "\n",
    "    # Crée le dossier de sortie s'il n'existe pas\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Récupère tous les fichiers JPEG\n",
    "    image_files = glob.glob(os.path.join(images_dir, \"*.jpg\"))\n",
    "\n",
    "    for image_path in image_files:\n",
    "        # Récupère le nom de base (sans extension)\n",
    "        base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        annotation_path = os.path.join(images_dir, base_name + \".txt\")\n",
    "\n",
    "        # Charge l'image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Erreur de chargement de l'image {image_path}\")\n",
    "            continue\n",
    "\n",
    "        height, width = img.shape[:2]\n",
    "\n",
    "        # Vérifie l'existence du fichier d'annotation correspondant\n",
    "        if not os.path.exists(annotation_path):\n",
    "            print(f\"Aucune annotation pour {image_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(annotation_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Pour chaque ligne (chaque bounding box)\n",
    "        for idx, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Format attendu : class x_center y_center width height (valeurs normalisées)\n",
    "            parts = line.split()\n",
    "            if len(parts) != 5:\n",
    "                print(f\"Format incorrect dans {annotation_path}: {line}\")\n",
    "                continue\n",
    "\n",
    "            class_id = parts[0]  # Utilisé pour nommer le fichier\n",
    "            x_center_norm, y_center_norm, width_norm, height_norm = map(float, parts[1:])\n",
    "\n",
    "            # Conversion en coordonnées absolues\n",
    "            x_center = x_center_norm * width\n",
    "            y_center = y_center_norm * height\n",
    "            box_width = width_norm * width\n",
    "            box_height = height_norm * height\n",
    "\n",
    "            # Calcul des coordonnées (en veillant à rester dans l'image)\n",
    "            x_min = int(max(x_center - box_width / 2, 0))\n",
    "            y_min = int(max(y_center - box_height / 2, 0))\n",
    "            x_max = int(min(x_center + box_width / 2, width))\n",
    "            y_max = int(min(y_center + box_height / 2, height))\n",
    "\n",
    "            # Extraction de la région d'intérêt\n",
    "            roi = img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            # Génération d'un identifiant unique\n",
    "            hash_object = hashlib.md5(f\"{base_name}_class{class_id}_roi{idx}\".encode())\n",
    "            identifiant = hash_object.hexdigest()[:8]  # 8 premiers caractères du hash\n",
    "\n",
    "            # Sauvegarde de la ROI avec un nom unique\n",
    "            output_filename = f\"class{class_id}_{identifiant}.jpg\"\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            cv2.imwrite(output_path, roi)\n",
    "\n",
    "            print(f\"ROI enregistrée: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II-Repartition des données en train et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "source_dir = \"InterestData\"\n",
    "\n",
    "train_dir = \"Dataset/train\"\n",
    "val_dir = \"Dataset/validation\"\n",
    "split_ratio = 0.8  # 80% train / 20% validation\n",
    "\n",
    "# Création des répertoires\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Dictionnaire pour regrouper les fichiers par classe\n",
    "class_files = {}\n",
    "\n",
    "# Remplir le dictionnaire avec les fichiers classés par classe\n",
    "for filename in os.listdir(source_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        try:\n",
    "            class_id = filename.split(\"_\")[0][5:]  # Extraction du class_id\n",
    "            class_files.setdefault(class_id, []).append(filename)\n",
    "        except:\n",
    "            print(f\"Format invalide pour {filename}\")\n",
    "            continue\n",
    "\n",
    "# Répartition stratifiée\n",
    "for class_id, files in class_files.items():\n",
    "    random.shuffle(files)  # Mélange aléatoire\n",
    "    split_idx = int(len(files) * split_ratio)\n",
    "    \n",
    "    # Déplacement vers train\n",
    "    for file in files[:split_idx]:\n",
    "        src = os.path.join(source_dir, file)\n",
    "        dst = os.path.join(train_dir, file)\n",
    "        shutil.move(src, dst)\n",
    "    \n",
    "    # Déplacement vers validation\n",
    "    for file in files[split_idx:]:\n",
    "        src = os.path.join(source_dir, file)\n",
    "        dst = os.path.join(val_dir, file)\n",
    "        shutil.move(src, dst)\n",
    "\n",
    "print(f\"Répartition terminée :\")\n",
    "print(f\"- Train: {len(os.listdir(train_dir))} images\")\n",
    "print(f\"- Validation: {len(os.listdir(val_dir))} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III-IMPLEMENTATION CONVNEXT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "random.seed(42)\n",
    "\n",
    "# Paramètres\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "num_classes = 3  # Adapté à vos 3 maladies\n",
    "\n",
    "# Fonctions utilitaires pour le pipeline\n",
    "\n",
    "def parse_image(filename):\n",
    "    \"\"\"Lit et redimensionne l'image.\"\"\"\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.image.resize(image, img_size)\n",
    "    # Prétraitement spécifique à ConvNeXtSmall\n",
    "    image = tf.keras.applications.convnext.preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "def get_label_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extrait le label à partir du nom du fichier.\n",
    "    Format attendu : \"class{id_class}_{unique_id}.jpg\"\n",
    "    \"\"\"\n",
    "    # Extraire le nom de fichier (sans chemin)\n",
    "    fname = tf.strings.split(filename, os.sep)[-1]\n",
    "    # Retirer le préfixe \"class\" et récupérer la partie avant le premier underscore\n",
    "    label_str = tf.strings.regex_replace(fname, \"^class\", \"\")\n",
    "    label_str = tf.strings.split(label_str, \"_\")[0]\n",
    "    # Convertir en entier\n",
    "    label = tf.strings.to_number(label_str, out_type=tf.int32)\n",
    "    return label\n",
    "\n",
    "def load_and_preprocess_image(filename):\n",
    "    \"\"\"Charge l'image et extrait le label one-hot.\"\"\"\n",
    "    image = parse_image(filename)\n",
    "    label = get_label_from_filename(filename)\n",
    "    # Encodage one-hot pour num_classes classes\n",
    "    label = tf.one_hot(label, depth=num_classes)\n",
    "    return image, label\n",
    "\n",
    "# Création des datasets pour \"train\" et \"test\"\n",
    "train_files = tf.data.Dataset.list_files(\"Dataset/train/*.jpg\", shuffle=True)\n",
    "test_files  = tf.data.Dataset.list_files(\"Dataset/validation/*.jpg\", shuffle=False)\n",
    "\n",
    "train_ds = train_files.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds  = test_files.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Regroupement en batches et optimisation par prefetch\n",
    "train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds  = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Création du modèle ConvNeXtSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Construction du modèle avec ConvNeXtSmall\n",
    "# -------------------------------------------\n",
    "\n",
    "# Charger la base pré-entraînée ConvNeXtSmall sans la tête (include_top=False)\n",
    "base_model = tf.keras.applications.ConvNeXtSmall(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(img_size[0], img_size[1], 3)\n",
    ")\n",
    "\n",
    "# Gel initial de la base pour entraîner uniquement la tête de classification\n",
    "base_model.trainable = False\n",
    "\n",
    "# Construction de la tête de classification\n",
    "inputs = tf.keras.Input(shape=(img_size[0], img_size[1], 3))\n",
    "x = base_model(inputs, training=False)  # On passe training=False pour un comportement stable\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compilation du modèle pour l'entraînement initial\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entraînement initial et Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Étape 2 et 3 : Entraînement initial et Fine-tuning\n",
    "# -------------------------------------------\n",
    "\n",
    "# Entraînement initial (seulement la nouvelle tête)\n",
    "initial_epochs = 20\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=initial_epochs,\n",
    "    validation_data=test_ds\n",
    ")\n",
    "\n",
    "# Fine-tuning : dégeler partiellement la base\n",
    "base_model.trainable = True\n",
    "# Par exemple, dégeler seulement les 20% dernières couches\n",
    "fine_tune_at = int(len(base_model.layers) * 0.8)\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompiler le modèle avec un taux d'apprentissage réduit\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "fine_tune_epochs = 30\n",
    "total_epochs = initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=initial_epochs,\n",
    "    validation_data=test_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model(\"Model/modele_convnext_small_mine_tf(e50).keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Prédire sur l'ensemble de test\n",
    "y_pred_prob = model.predict(test_ds)  # Shape: (nb_exemples, num_classes)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# 2. Extraire les vraies étiquettes à partir du dataset test\n",
    "y_true = []\n",
    "for images, labels in test_ds:\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# 3. Calculer et afficher le rapport de classification\n",
    "target_names = [\"class0\", \"class1\", \"class2\"]\n",
    "report = classification_report(y_true, y_pred, target_names=target_names)\n",
    "print(report)\n",
    "\n",
    "# 4. Calculer la matrice de confusion et l'afficher graphiquement\n",
    "\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Calculer les pourcentages par classe réelle (normalisation par ligne)\n",
    "cm_sum = cm.sum(axis=1, keepdims=True)\n",
    "cm_perc = (cm / cm_sum.astype(float)) * 100\n",
    "\n",
    "# Créer une matrice d'annotations combinant compte et pourcentage\n",
    "labels = np.empty_like(cm, dtype=object)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        labels[i, j] = f\"{cm[i,j]}\\n({cm_perc[i,j]:.1f}%)\"\n",
    "\n",
    "# Afficher la heatmap avec les annotations\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=labels, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Classe prédite\")\n",
    "plt.ylabel(\"Classe réelle\")\n",
    "plt.title(\"Matrice de confusion modèle SVM\")\n",
    "plt.savefig(\"svg/ConfusionMatrix.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pickle\n",
    "\n",
    "with open('Model/history(e50).pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "with open('Model/history_fine(e50).pkl', 'wb') as f:\n",
    "    pickle.dump(history_fine.history, f)\n",
    "\n",
    "# Sauvegarde au format SavedModel (par défaut)\n",
    "model.save(\"Model/modele_convnext_small_mine_tf(e50).keras\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "# Charger l'historique d'entraînement\n",
    "with open('Model/history.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "with open('Model/history_fine.pkl', 'rb') as f:\n",
    "    history_fine = pickle.load(f)\n",
    "\"\"\"\n",
    "\n",
    "# Fusionner les historiques (entraînement initial + fine-tuning)\n",
    "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
    "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
    "loss = history.history['loss'] + history_fine.history['loss']\n",
    "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
    "precision = history.history['precision'] + history_fine.history['precision_1']\n",
    "val_precision = history.history['val_precision'] + history_fine.history['val_precision_1']\n",
    "recall = history.history['recall'] + history_fine.history['recall_1']\n",
    "val_recall = history.history['val_recall'] + history_fine.history['val_recall_1']\n",
    "\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "initial_epochs = len(history.history['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Création du dossier s'il n'existe pas\n",
    "output_dir = \"svg\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Fonction pour sauvegarder et afficher une figure\n",
    "def save_and_show_plot(x, y_train, y_val, title, ylabel, filename):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(x, y_train, label=\"Training \" + ylabel)\n",
    "    plt.plot(x, y_val, label=\"Validation \" + ylabel)\n",
    "    plt.axvline(x=initial_epochs-1, color='r', linestyle='--', label=\"Début Fine-Tuning\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Époques\")\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    # Sauvegarde en SVG\n",
    "    path = os.path.join(output_dir, filename)\n",
    "    plt.savefig(path, format=\"svg\")\n",
    "    print(f\"Graphique sauvegardé : {path}\")\n",
    "\n",
    "    # Affichage\n",
    "    plt.show()\n",
    "\n",
    "# Sauvegarder et afficher chaque courbe individuellement\n",
    "save_and_show_plot(epochs_range, acc, val_acc, \"Courbe d'Accuracy\", \"Accuracy\", \"accuracy.svg\")\n",
    "save_and_show_plot(epochs_range, loss, val_loss, \"Courbe de Perte\", \"Loss\", \"loss.svg\")\n",
    "save_and_show_plot(epochs_range, precision, val_precision, \"Courbe de Précision\", \"Precision\", \"precision.svg\")\n",
    "save_and_show_plot(epochs_range, recall, val_recall, \"Courbe de Rappel (Recall)\", \"Recall\", \"recall.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from itertools import cycle\n",
    "\n",
    "# Prédictions sous forme de probabilités\n",
    "y_scores = model.predict(test_ds)\n",
    "y_true = np.array(y_true)  # Convertir en array numpy\n",
    "\n",
    "# Tracer la courbe ROC pour chaque classe\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = cycle([\"blue\", \"red\", \"green\"])\n",
    "for i, color in zip(range(num_classes), colors):\n",
    "    fpr, tpr, _ = roc_curve(y_true == i, y_scores[:, i])  # Calcul du FPR et TPR\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=color, label=f\"Classe {i} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\")  # Ligne de référence\n",
    "plt.xlabel(\"Taux de faux positifs\")\n",
    "plt.ylabel(\"Taux de vrais positifs\")\n",
    "plt.title(\"Courbes ROC par classe\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "# Sauvegarde en SVG\n",
    "path = os.path.join(output_dir, \"ROC-AUC.svg\")\n",
    "plt.savefig(path, format=\"svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des erreurs de classification avec ConvNext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Récupération de tous les fichiers .jpg du dossier validation\n",
    "file_paths = glob.glob(\"Dataset/validation/*.jpg\")\n",
    "\n",
    "# Optionnel : trier la liste pour avoir un ordre constant\n",
    "# file_paths.sort()\n",
    "\n",
    "print(\"Nombre d'images dans validation :\", len(file_paths))\n",
    "print(file_paths[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de DataFrame regroupant les informations\n",
    "import pandas as pd\n",
    "df_errors = pd.DataFrame({\n",
    "    \"file_path\": file_paths,\n",
    "    \"true_label\": y_true,\n",
    "    \"pred_label\": y_pred,\n",
    "    \"confidence\": np.max(y_pred_prob, axis=1)\n",
    "})\n",
    "\n",
    "# Filtrer les exemples mal classés\n",
    "df_errors = df_errors[df_errors[\"true_label\"] != df_errors[\"pred_label\"]]\n",
    "print(\"Nombre d'erreurs :\", len(df_errors))\n",
    "print(df_errors.head())\n",
    "\n",
    "# visualiser certaines images mal classées\n",
    "import cv2\n",
    "\n",
    "for idx, row in df_errors.head(2).iterrows():\n",
    "    img = cv2.imread(row[\"file_path\"])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Vrai: {target_names[int(row['true_label'])]} - Prédit: {target_names[int(row['pred_label'])]}, Confiance: {row['confidence']:.2f}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Création du modèle ConvNext(ConvNeXtSmall) + SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.convnext import ConvNeXtSmall, preprocess_input\n",
    "\n",
    "# Fonction pour charger les images et extraire les labels à partir du nom\n",
    "def load_images_and_labels(directory, target_size=(224, 224)):\n",
    "    img_list = []\n",
    "    labels = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.lower().endswith('.jpg'):\n",
    "            # Extraction du label à partir du nom de fichier\n",
    "            # Format attendu : \"class{id_class}_{unique_id}.jpg\"\n",
    "            base_name = os.path.splitext(file_name)[0]  # retire l'extension\n",
    "            # Par exemple, \"class0_12345\" -> on récupère la première partie avant le '_'\n",
    "            label_str = base_name.split('_')[0]\n",
    "            # Extraire l'id de classe en retirant le préfixe \"class\"\n",
    "            label = int(label_str.replace(\"class\", \"\"))\n",
    "            labels.append(label)\n",
    "            \n",
    "            # Chargement et prétraitement de l'image\n",
    "            img_path = os.path.join(directory, file_name)\n",
    "            img = image.load_img(img_path, target_size=target_size)\n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array = preprocess_input(img_array)\n",
    "            img_list.append(img_array)\n",
    "    return np.array(img_list), np.array(labels)\n",
    "\n",
    "# Chemins vers les dossiers d'images\n",
    "train_dir = \"Dataset/train/\"\n",
    "val_dir   = \"Dataset/validation/\"\n",
    "\n",
    "# Chargement des images et des labels\n",
    "X_train, y_train = load_images_and_labels(train_dir, target_size=(224, 224))\n",
    "X_val, y_val     = load_images_and_labels(val_dir, target_size=(224, 224))\n",
    "\n",
    "print(f\"Nombre d'images d'entraînement : {X_train.shape[0]}\")\n",
    "print(f\"Nombre d'images de validation : {X_val.shape[0]}\")\n",
    "\n",
    "# base_model = ConvNeXtTiny(weights='imagenet', include_top=False, pooling='avg')\n",
    "base_model = tf.keras.applications.ConvNeXtSmall(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg',\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Gel initial de la base pour entraîner uniquement la tête de classification\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "# Fonction d'extraction des features\n",
    "def extract_features(images, model):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        img_batch = np.expand_dims(img, axis=0)  # Ajouter la dimension batch\n",
    "        feat = model.predict(img_batch)\n",
    "        features.append(feat.squeeze())\n",
    "    return np.array(features)\n",
    "\n",
    "# Extraction des features pour les ensembles train et validation\n",
    "features_train = extract_features(X_train, base_model)\n",
    "features_val   = extract_features(X_val, base_model)\n",
    "\n",
    "print(f\"Dimension des features extraites : {features_train.shape[1]}\")\n",
    "\n",
    "# Sauvegarde des features et labels pour usage ultérieur\n",
    "np.save(\"Extraction/features_train.npy\", features_train)\n",
    "np.save(\"Extraction/labels_train.npy\", y_train)\n",
    "np.save(\"Extraction/features_val.npy\", features_val)\n",
    "np.save(\"Extraction/labels_val.npy\", y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Chargement des features et labels\n",
    "features_train = np.load(\"Extraction/features_train.npy\")\n",
    "y_train = np.load(\"Extraction/labels_train.npy\")\n",
    "features_val = np.load(\"Extraction/features_val.npy\")\n",
    "y_val = np.load(\"Extraction/labels_val.npy\")\n",
    "###########################################\n",
    "# Définition d'un wrapper de noyau paramétrable\n",
    "###########################################\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics.pairwise import euclidean_distances, linear_kernel\n",
    "\n",
    "class HybridKernel(BaseEstimator):\n",
    "    def _init_(self, gamma=0.1, alpha=0.5, beta=0.5):\n",
    "        self.gamma = gamma  # Paramètre de la composante non-linéaire\n",
    "        self.alpha = alpha  # Poids composante inverse-euclidienne\n",
    "        self.beta = beta    # Poids composante linéaire\n",
    "        \n",
    "    def _call_(self, X, Y=None):\n",
    "        if Y is None:\n",
    "            Y = X\n",
    "            \n",
    "        # Calcul des deux composantes du noyau\n",
    "        distance = euclidean_distances(X, Y, squared=True)\n",
    "        inv_euclidean = 1 / (1 + self.gamma * distance)\n",
    "        linear_comp = linear_kernel(X, Y)\n",
    "        \n",
    "        # Combinaison pondérée\n",
    "        K = self.alpha * inv_euclidean + self.beta * linear_comp\n",
    "        \n",
    "        # Ajustement spectral uniquement pour la matrice d'entraînement\n",
    "        if X is Y or Y is None:\n",
    "            eigenvalues = np.linalg.eigvalsh(K)\n",
    "            min_eig = np.min(eigenvalues)\n",
    "            if min_eig < 1e-8:\n",
    "                K += np.eye(K.shape[0]) * (abs(min_eig) + 1e-8)\n",
    "                \n",
    "        return K\n",
    "\n",
    "###########################################\n",
    "# Pipeline d'optimisation avec GridSearchCV\n",
    "###########################################\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Création du modèle avec noyau paramétrable\n",
    "kernel = HybridKernel()\n",
    "svm = SVC(kernel=kernel, decision_function_shape='ovr')\n",
    "\n",
    "# Grille d'hyperparamètres à explorer\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],                  # Paramètre de régularisation SVM\n",
    "    'kernel__gamma': [0.01, 0.1, 1.0],   # Paramètre du noyau non-linéaire\n",
    "    'kernel__alpha': [0.3, 0.5, 0.7,0.8,0.9],    # Poids composante non-linéaire\n",
    "    'kernel__beta': [0.3, 0.5, 0.7,0.8,0.9]      # Poids composante linéaire\n",
    "}\n",
    "\n",
    "# Configuration de la validation croisée\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                # 5 folds de validation croisée\n",
    "    n_jobs=-1,           # Utilisation de tous les cœurs CPU\n",
    "    scoring='accuracy',  # Métrique d'évaluation\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Exécution de la recherche sur les données d'entraînement\n",
    "grid_search.fit(features_train, y_train)\n",
    "\n",
    "# Résultats\n",
    "print(f\"Meilleure combinaison d'hyperparamètres : {grid_search.best_params_}\")\n",
    "print(f\"Meilleure précision cross-val : {grid_search.best_score_*100:.2f}%\")\n",
    "\n",
    "# Évaluation sur le jeu de validation\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred = best_svm.predict(features_val)\n",
    "y_pred_prob=best_svm.predict_proba(features_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Précision sur la validation : {accuracy*100:.2f}%\")\n",
    "\n",
    "# Sauvegarde du meilleur modèle\n",
    "import joblib\n",
    "joblib.dump(best_svm, 'Model/best_svm_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculer la matrice de confusion\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# Calculer les pourcentages par classe réelle (normalisation par ligne)\n",
    "cm_sum = cm.sum(axis=1, keepdims=True)\n",
    "cm_perc = (cm / cm_sum.astype(float)) * 100\n",
    "\n",
    "# Créer une matrice d'annotations combinant compte et pourcentage\n",
    "labels = np.empty_like(cm, dtype=object)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        labels[i, j] = f\"{cm[i,j]}\\n({cm_perc[i,j]:.1f}%)\"\n",
    "\n",
    "# Afficher la heatmap avec les annotations\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=labels, fmt=\"\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Classe prédite\")\n",
    "plt.ylabel(\"Classe réelle\")\n",
    "plt.title(\"Matrice de confusion modèle SVM\")\n",
    "plt.savefig(\"svg/ConfusionMatrixSVM.svg\", format=\"svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des erreurs de classification avec ConvNext+SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Récupération de tous les fichiers .jpg du dossier validation\n",
    "file_paths = glob.glob(\"Dataset/validation/*.jpg\")\n",
    "\n",
    "# Optionnel : trier la liste pour avoir un ordre constant\n",
    "# file_paths.sort()\n",
    "\n",
    "print(\"Nombre d'images dans validation :\", len(file_paths))\n",
    "print(file_paths[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de DataFrame regroupant les informations\n",
    "import pandas as pd\n",
    "df_errors = pd.DataFrame({\n",
    "    \"file_path\": file_paths,\n",
    "    \"true_label\": y_val,\n",
    "    \"pred_label\": y_pred,\n",
    "    \"confidence\": np.max(y_pred_prob, axis=1)\n",
    "})\n",
    "\n",
    "# Filtrer les exemples mal classés\n",
    "df_errors = df_errors[df_errors[\"true_label\"] != df_errors[\"pred_label\"]]\n",
    "print(\"Nombre d'erreurs :\", len(df_errors))\n",
    "print(df_errors.head())\n",
    "\n",
    "# visualiser certaines images mal classées\n",
    "import cv2\n",
    "\n",
    "for idx, row in df_errors.head(2).iterrows():\n",
    "    img = cv2.imread(row[\"file_path\"])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Vrai: {target_names[int(row['true_label'])]} - Prédit: {target_names[int(row['pred_label'])]}, Confiance: {row['confidence']:.2f}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV-PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-ALGORITHME DE DETECTION YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Charger le modèle entraîné\n",
    "model0 = YOLO(\"Model/model_cacao.pt\")\n",
    "\n",
    "# Charger l'image sur laquelle tu veux détecter les fèves\n",
    "image_path = \"image04.jpg\"\n",
    "results = model0(image_path)  # Détection sur l'image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Charger l'image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Vérifier si l'image est correctement chargée\n",
    "if image is None:\n",
    "    print(f\"Erreur : Impossible de charger l'image à partir de {image_path}\")\n",
    "    exit()\n",
    "\n",
    "# Dessiner les boîtes de détection\n",
    "for result in results:\n",
    "    boxes = result.boxes.xyxy  # Coordonnées des bounding boxes [x1, y1, x2, y2]\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box)  # Conversion en entier\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Dessiner la boîte verte\n",
    "\n",
    "# Afficher l'image avec OpenCV\n",
    "cv2.imshow(\"Détection des fèves\", image)\n",
    "cv2.waitKey(0)  # Attendre une touche pour fermer la fenêtre\n",
    "cv2.destroyAllWindows()  # Fermer toutes les fenêtres OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLASSIFICATION D'UNE IMAGE AVEC ConvNeXtSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --------------------------\n",
    "# 1. Chargement des modèles\n",
    "# --------------------------\n",
    "\n",
    "# Charger le modèle YOLOv8 personnalisé pour la détection des fèves de cacao.\n",
    "model_yolo = YOLO(\"Model/model_cacao.pt\")\n",
    "\n",
    "# Charger le modèle de classification basé sur ConvNeXtSmall (sauvegardé au format SavedModel)\n",
    "classifier = tf.keras.models.load_model(\"Model/modele_convnext_small_mine_tf(e50).keras\")\n",
    "\n",
    "# Définir les noms des classes de maladies (3 classes dans cet exemple)\n",
    "target_names = [\"class0\", \"class1\", \"class2\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 2. Importation et détection avec YOLOv8\n",
    "# --------------------------\n",
    "\n",
    "# Charger l'image contenant plusieurs fèves de cacao\n",
    "img_path = \"image01.jpg\"  # Remplacez par le chemin vers votre image\n",
    "orig_img = cv2.imread(img_path)\n",
    "if orig_img is None:\n",
    "    raise ValueError(f\"Erreur lors du chargement de l'image {img_path}\")\n",
    "\n",
    "# Convertir l'image en RGB pour l'affichage avec Matplotlib\n",
    "img_rgb = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Appliquer le modèle YOLOv8 pour détecter les fèves\n",
    "results = model_yolo(orig_img)\n",
    "\n",
    "# YOLOv8 retourne une liste de résultats (pour chaque image, ici une seule)\n",
    "# Les bounding boxes sont accessibles via results[0].boxes\n",
    "boxes = results[0].boxes  # Contient xyxy, conf, et cls (classe prédite par YOLO)\n",
    "# Convertir en numpy arrays\n",
    "xyxy = boxes.xyxy.cpu().numpy()  # Coordonnées : [x1, y1, x2, y2]\n",
    "confidences = boxes.conf.cpu().numpy()  # Score de confiance\n",
    "# (La classe prédite par YOLO est généralement utile pour d'autres tâches, ici nous faisons la classification via ConvNeXt)\n",
    "    \n",
    "# Filtrer les détections avec un seuil de confiance (par exemple 0.5)\n",
    "detections_list = []\n",
    "confidence_threshold = 0.5\n",
    "for i in range(len(xyxy)):\n",
    "    if confidences[i] < confidence_threshold:\n",
    "        continue\n",
    "    x1, y1, x2, y2 = map(int, xyxy[i])\n",
    "    detections_list.append((x1, y1, x2, y2, confidences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# --------------------------\n",
    "# 3. Classification et affichage des résultats avec légende\n",
    "# --------------------------\n",
    "\n",
    "# Copie de l'image pour annoter les résultats\n",
    "output_img = img_rgb.copy()\n",
    "\n",
    "# Pour chaque détection (fève détectée), extraire la ROI, la classifier et afficher le résultat\n",
    "for (x1, y1, x2, y2, conf_det) in detections_list:\n",
    "    # Extraire la région d'intérêt correspondant à la fève\n",
    "    roi = orig_img[y1:y2, x1:x2]\n",
    "    \n",
    "    # Préparation de la ROI pour le modèle de classification :\n",
    "    #  - Redimensionner à 224x224 (taille d'entrée du modèle)\n",
    "    #  - Convertir BGR -> RGB\n",
    "    roi_resized = cv2.resize(roi, (224, 224))\n",
    "    roi_rgb = cv2.cvtColor(roi_resized, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Prétraitement pour ConvNeXtSmall (normalisation)\n",
    "    roi_preprocessed = tf.keras.applications.convnext.preprocess_input(np.array(roi_rgb, dtype=np.float32))\n",
    "    roi_preprocessed = np.expand_dims(roi_preprocessed, axis=0)  # Ajout de la dimension batch\n",
    "    \n",
    "    # Prédiction de la classe de maladie pour la ROI\n",
    "    pred_prob = classifier.predict(roi_preprocessed)  # Renvoie un vecteur de probabilités de forme (1, 3)\n",
    "    pred_class = np.argmax(pred_prob, axis=1)[0]\n",
    "    confidence_class = np.max(pred_prob)\n",
    "    \n",
    "    # Création du label à afficher : nom de la maladie et score de confiance\n",
    "    label_text = f\"{target_names[pred_class]} ({confidence_class:.2f})\"\n",
    "    \n",
    "    # Dessiner la bounding box avec une épaisseur plus grande\n",
    "    cv2.rectangle(output_img, (x1, y1), (x2, y2), (255, 0, 0), 4)  # Epaisseur du cadre augmentée à 4\n",
    "    \n",
    "    # Augmenter la taille de la police dans cv2.putText()\n",
    "    cv2.putText(output_img, label_text, (x1, max(y1 - 10, 0)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)  # Taille de police 1 et épaisseur de texte 3\n",
    "\n",
    "# Affichage de l'image annotée avec Matplotlib\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(output_img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Détection (YOLOv8) et classification (ConvNeXt) des fèves de cacao\")\n",
    "\n",
    "# Légende : Associer chaque classe à une couleur\n",
    "class_labels = [\"Sana\", \"Fito\", \"Monilia\"]\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]  # Rouge, Vert, Bleu\n",
    "\n",
    "# Création de la légende\n",
    "import matplotlib.patches as mpatches\n",
    "legend_handles = [mpatches.Patch(color=np.array(color)/255, label=label) for label, color in zip(class_labels, colors)]\n",
    "plt.legend(handles=legend_handles, loc=\"lower left\", fontsize=12, title=\"Classes\")\n",
    "\n",
    "# Afficher l'image\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
